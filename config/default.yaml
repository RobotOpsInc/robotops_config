# Robot Agent Configuration

# ===== CONFIGURATION VERSION =====
#
# Schema version for configuration compatibility checking.
# Components validate that the major version matches their supported version.
# All v1.x.x configs are compatible with all v1.x.x components.
#
# Format: semver (MAJOR.MINOR.PATCH)
# - MAJOR: Breaking changes (field removal, type changes, required fields)
# - MINOR: New optional fields (backward compatible)
# - PATCH: Documentation or default value changes
schema_version: "0.2.0"

# ===== ROBOT IDENTITY =====
#
# Uniquely identifies this robot within your organization for billing (robot-hours)
# and fleet organization. All fields are optional with sensible defaults/auto-generation.
#
# robot:
#   # Unique identifier for this robot (alphanumeric, hyphens, underscores; 3-128 chars)
#   # If not provided, auto-generates as "{hostname}-{short_hash}" with a warning
#   # Auto-generated IDs are persisted at /var/lib/robotops/robot_id for stability
#   # Environment variable: ROBOT_OPS_AGENT_ROBOT_ID
#   # id: "amr-warehouse-007"
#
#   # Human-friendly display name (optional, no validation constraints)
#   # Environment variable: ROBOT_OPS_AGENT_ROBOT_NICKNAME
#   # nickname: "Warehouse AMR #7"
#
#   # Fleet assignment (defaults to "default" if not specified)
#   # Environment variable: ROBOT_OPS_AGENT_ROBOT_FLEET_ID
#   # fleet_id: "warehouse-fleet-east"
#
#   # Deployment environment (defaults to "development")
#   # Common values: "development", "staging", "production", "simulation"
#   # No strict validation - use any custom value
#   # Environment variable: ROBOT_OPS_AGENT_ENVIRONMENT
#   # environment: "production"
#
#   # Custom tags for filtering and organization
#   # Keys: alphanumeric, hyphens, underscores; 1-64 characters
#   # Values: any string; 1-256 characters
#   # Maximum 20 tags per robot
#   # Environment variable: ROBOT_OPS_AGENT_TAGS (JSON, e.g., '{"region":"us-west-2"}')
#   # tags:
#   #   region: us-west-2
#   #   building: warehouse-7
#   #   team: navigation
#   #   hardware_revision: v2.1

# ===== DEPLOYMENT VERSION =====
#
# Specify the version of your ROS2 deployment (your robot software, not the agent).
# This enables correlating issues with specific software releases in your fleet.
# Field is optional - if not provided, it will be left empty.
#
# deployment:
#   # Software version identifier (no format validation - use any scheme)
#   # Supports semver: "2.1.0"
#   # Supports git SHA: "a1b2c3d"
#   # Supports hybrid: "2.1.0-a1b2c3d"
#   # Supports custom: "release-2025-01-15"
#   #
#   # Environment variable templating: Use ${VAR_NAME} to inject from environment
#   # Example: version: "${GIT_SHA}" (resolves from GIT_SHA env var)
#   # Example: version: "v2.1.0-${BUILD_NUMBER}"
#   #
#   # Environment variable override: ROBOTOPS_DEPLOYMENT_VERSION (takes precedence)
#   # version: "2.1.0"

# ===== AUTHENTICATION =====
#
# API key authentication for RobotOps backend communication.
# The agent uses the API key to authenticate gRPC connections and resolve the
# organization ID on first successful connection.
#
# SECURITY: API keys are NEVER logged. Config files containing API keys should
# have restricted permissions (chmod 600 recommended).
#
# auth:
#   # API key for RobotOps backend (starts with "sk_")
#   # Environment variable: ROBOTOPS_API_KEY (highest precedence)
#   # api_key: "sk_your_api_key_here"
#
#   # Alternative: Read API key from file (useful for secrets management)
#   # Takes precedence over inline api_key but not over ROBOTOPS_API_KEY
#   # Environment variable: ROBOT_OPS_AGENT_AUTH_API_KEY_FILE (currently not implemented)
#   # api_key_file: "/run/secrets/robotops_api_key"

# ===== BACKEND CONFIGURATION =====
#
# RobotOps backend endpoint and retry configuration.
#
# backend:
#   # Backend gRPC endpoint URL
#   # TODO(ROB-42): Update with actual production endpoint URL
#   # Default: "https://api.robotops.com"
#   # Environment variable: ROBOT_OPS_AGENT_BACKEND_URL
#   # url: "https://api.robotops.com"
#
#   # Heartbeat interval for connectivity checks (seconds)
#   # The agent periodically sends heartbeat requests to detect connectivity
#   # issues and API key revocations
#   # Default: 30 seconds
#   # Environment variable: ROBOT_OPS_AGENT_BACKEND_HEARTBEAT_INTERVAL_SECS
#   # heartbeat_interval_secs: 30
#
#   # Maximum retry attempts before entering offline mode
#   # Default: 5
#   # Environment variable: ROBOT_OPS_AGENT_BACKEND_MAX_RETRY_ATTEMPTS
#   # max_retry_attempts: 5
#
#   # Initial backoff duration for exponential backoff (seconds)
#   # Doubles with each retry up to max_backoff_secs
#   # Default: 1 second
#   # Environment variable: ROBOT_OPS_AGENT_BACKEND_INITIAL_BACKOFF_SECS
#   # initial_backoff_secs: 1
#
#   # Maximum backoff duration for exponential backoff (seconds)
#   # Default: 60 seconds
#   # Environment variable: ROBOT_OPS_AGENT_BACKEND_MAX_BACKOFF_SECS
#   # max_backoff_secs: 60

# ===== DISTRIBUTED TRACING =====
#
# Configuration for rmw_robotops distributed tracing and robot_agent trace correlation.
#
# Distributed tracing creates causal links between publish/subscribe/service/action
# calls across ROS2 nodes, enabling end-to-end latency analysis and debugging.
# This is separate from message capture (subscriptions:) which controls payload storage.
#
# ## How It Works
#
# 1. rmw_robotops intercepts ROS2 communication at the RMW layer
# 2. On publish: mints trace_id (if sampled), attaches context, emits TraceEvent
# 3. On subscribe: extracts context (FastDDS) or correlates (fallback), emits TraceEvent
# 4. robot_agent subscribes to /robotops/trace_events, correlates, exports to OTel
#
# ## Relationship to subscriptions:
#
# - subscriptions.topic_overrides: Controls message PAYLOAD capture for replay
# - tracing.trace_rate.overrides: Controls TRACE CONTEXT creation for causal analysis
#
# A message can be captured but not traced, traced but not captured, both, or neither.

tracing:
  # ===== MASTER ENABLE SWITCH =====
  #
  # When false, rmw_robotops acts as a pure passthrough to the underlying RMW
  # with zero overhead. No trace context is created, no TraceEvents are emitted.
  #
  # Use this to completely disable tracing without uninstalling rmw_robotops.
  #
  # Default: true
  # Environment variable: ROBOTOPS_TRACING_ENABLED
  enabled: true

  # ===== UNDERLYING RMW =====
  #
  # The RMW implementation that rmw_robotops wraps and delegates to.
  # rmw_robotops intercepts calls for tracing, then forwards to this RMW
  # for actual DDS communication.
  #
  # Common values:
  # - "rmw_fastrtps_cpp" (Fast DDS, default for ROS2 Jazzy)
  # - "rmw_cyclonedds_cpp" (Cyclone DDS)
  # - "rmw_connextdds" (RTI Connext)
  #
  # Default: "rmw_fastrtps_cpp"
  # Environment variable: ROBOTOPS_UNDERLYING_RMW
  underlying_rmw: "rmw_fastrtps_cpp"

  # ===== HEAD-BASED TRACE SAMPLING =====
  #
  # Controls whether rmw_robotops creates a new trace for a given publish.
  # This is "head-based" sampling: the decision is made at trace creation time
  # (the "head" of the trace), and all downstream spans inherit that decision.
  #
  # This is different from subscriptions.topic_overrides which controls message
  # payload capture. A high-frequency sensor might have:
  # - trace_rate: 0.01 (1% of publishes start a trace)
  # - subscriptions capture: 2 Hz (payloads stored for replay)
  #
  # The trace_rate controls distributed tracing overhead and trace storage volume.
  # The subscriptions capture controls message payload storage volume.
  trace_rate:
    # Default sampling rate for all topics (0.0 - 1.0).
    #
    # - 1.0: Every publish creates a trace (100% sampling)
    # - 0.1: 10% of publishes create traces
    # - 0.0: No traces created (effectively disables tracing)
    #
    # For most robots, 1.0 is fine. Reduce for high-frequency sensors or
    # when trace storage volume is a concern.
    #
    # Default: 1.0
    default: 1.0

    # Per-topic sampling rate overrides (first matching pattern wins).
    #
    # Use this to reduce trace volume for high-frequency topics while keeping
    # full tracing for critical control paths.
    #
    # Pattern syntax: regex (must match full topic name)
    # Rate: 0.0 - 1.0
    #
    # Example:
    # overrides:
    #   # Camera images: trace 1% (high bandwidth, low value per trace)
    #   - pattern: "^/camera/.*/image_raw$"
    #     rate: 0.01
    #
    #   # LiDAR scans: trace 10%
    #   - pattern: "^/scan$"
    #     rate: 0.1
    #
    #   # Control commands: always trace (critical path)
    #   - pattern: "^/cmd_vel$"
    #     rate: 1.0
    #
    #   # Diagnostics: always trace
    #   - pattern: "^/diagnostics$"
    #     rate: 1.0
    #
    # Default: [] (no overrides, use default rate)
    # overrides: []

  # ===== CROSS-PROCESS CORRELATION =====
  #
  # Configuration for robot_agent's trace correlation engine.
  # When FastDDS SHM metadata extraction is unavailable, the agent falls back
  # to timestamp-based correlation to match publish events with subscribe events.
  #
  # These settings control the fallback correlation behavior.
  correlation:
    # Maximum time difference (nanoseconds) between publish and subscribe timestamps
    # for them to be considered the same message.
    #
    # Larger values: more lenient matching, may create false correlations
    # Smaller values: stricter matching, may miss valid correlations
    #
    # 10ms works well for most robots. Increase if you see missed correlations
    # in high-latency scenarios (e.g., network-bridged robots).
    #
    # Default: 10000000 (10ms)
    timestamp_tolerance_ns: 10000000

    # How long (seconds) to keep publish events in the correlation window.
    #
    # The agent maintains a sliding window of recent publish events to match
    # against incoming subscribe events. Longer windows use more memory but
    # handle delayed message delivery better.
    #
    # Default: 30 seconds
    window_secs: 30

    # Use content hash (xxHash64 of first 64 bytes) to disambiguate when multiple
    # publish events have similar timestamps.
    #
    # When enabled, correlation uses (topic, timestamp, hash) instead of just
    # (topic, timestamp). This prevents false correlations when the same topic
    # has multiple publishers at high frequency.
    #
    # Disable only if hash computation overhead is problematic (very unlikely).
    #
    # Default: true
    hash_enabled: true

  # ===== CLOCK SYNCHRONIZATION =====
  #
  # Distributed tracing relies on synchronized clocks across nodes.
  # These settings control clock skew detection and diagnostics.
  #
  # Clock skew can cause:
  # - Incorrect span ordering in trace visualization
  # - Failed correlations (if skew exceeds timestamp_tolerance_ns)
  # - Misleading latency measurements
  clock:
    # Maximum acceptable clock skew (nanoseconds) before warning.
    #
    # rmw_robotops periodically checks clock consistency and emits warnings
    # when skew exceeds this threshold. The check compares system clock
    # against ROS time and against other nodes (via trace event timestamps).
    #
    # Default: 10000000 (10ms)
    max_acceptable_skew_ns: 10000000

    # How often (seconds) to perform clock synchronization checks.
    #
    # Default: 60 seconds
    check_interval_secs: 60

  # ===== DIAGNOSTICS PUBLISHING =====
  #
  # rmw_robotops can publish periodic health metrics to a ROS2 topic for
  # monitoring and debugging. robot_agent subscribes to this topic and
  # exports diagnostics alongside traces.
  #
  # Published to: /robotops/diagnostics (DiagnosticsReport.msg)
  diagnostics:
    # Enable/disable diagnostics topic publishing.
    #
    # Default: true
    enabled: true

    # How often (seconds) to publish diagnostics.
    #
    # Lower values: more granular monitoring, slightly more overhead
    # Higher values: less overhead, coarser monitoring
    #
    # Default: 10 seconds
    interval_secs: 10

  # ===== PERFORMANCE TUNING =====
  #
  # Advanced settings for rmw_robotops performance. Most users should not
  # need to change these defaults.
  performance:
    # Internal queue depth for trace events before publishing to ROS2 topic.
    #
    # rmw_robotops buffers trace events in a lock-free queue to avoid blocking
    # the ROS2 publish/subscribe hot path. If the queue fills up (agent not
    # consuming fast enough), oldest events are dropped.
    #
    # Increase if you see "trace event queue full" warnings.
    # Decrease to reduce memory footprint on constrained devices.
    #
    # Default: 1024
    queue_size: 1024

    # Auto-disable tracing after this many consecutive failures.
    #
    # If trace event publishing fails repeatedly (e.g., topic not available),
    # rmw_robotops automatically disables tracing to prevent log spam and
    # wasted resources. Manual re-enable requires restart.
    #
    # Set to 0 to disable auto-disable (not recommended).
    #
    # Default: 100
    failure_threshold: 100

discovery:
  # How often to poll for new topics/nodes (seconds)
  poll_interval_secs: 5

  # Topic filters (regex patterns, optional)
  # If specified, only matching topics are subscribed
  # topic_filters:
  #   - "^/camera/.*"
  #   - "^/diagnostics$"

  # Topic blacklist (regex patterns, optional)
  # topic_blacklist:
  #   - "^/rosout$"

metrics:
  # ===== METRICS COLLECTION =====
  #
  # The agent periodically emits time-series metrics snapshots as telemetry envelopes.
  # Metrics are enabled by default and are designed to degrade gracefully when a given
  # metric is unavailable on the host (e.g., no thermal sensors, no battery).

  # Master enable switch for all metrics
  enabled: true

  # Emit metrics snapshots at this interval (seconds)
  collection_interval_secs: 10

  # Re-check optional sensors (battery/thermal) at this interval (seconds)
  autodetect_interval_secs: 60

  # Individual metric categories (all enabled by default)
  topic_metrics: true
  action_service_metrics: true
  infrastructure_metrics: true
  process_metrics: true
  temperature_metrics: true
  battery_metrics: true
  network_quality_metrics: true

  # Per-process metrics settings
  process_top_n: 25
  include_process_cmdline: false

subscriptions:
  # ===== ADAPTIVE CAPTURE STRATEGIES (NEW) =====

  # Default capture strategy (plug-and-play behavior)
  # Options: adaptive | all | on_change | rate_limit | sample
  # Default: adaptive (automatic bandwidth-based optimization)
  default_strategy: "adaptive"

  # Default target Hz for rate limiting (when not using adaptive)
  # Used when default_strategy is "rate_limit" or as fallback
  default_target_hz: 10.0

  # Adaptive rate limiting configuration
  # Used when default_strategy is "adaptive"
  adaptive:
    # Topics above this bandwidth are considered high-bandwidth sensors
    # Default: 1048576 bytes/sec (1 MB/s)
    # Example: camera at 30 Hz × 1 MB = 30 MB/s
    high_bandwidth_threshold_bps: 1048576

    # Downsample high-bandwidth topics to this rate
    # Default: 2.0 Hz
    high_bandwidth_target_hz: 2.0

    # Topics below this bandwidth are considered low-bandwidth state
    # Default: 10240 bytes/sec (10 KB/s)
    # Example: battery status at 1 Hz × 100 bytes = 100 B/s
    low_bandwidth_threshold_bps: 10240

    # Strategy for low-bandwidth topics
    # Default: on_change (only capture when values change)
    low_bandwidth_strategy: "on_change"

    # Medium-bandwidth topics use this rate
    # Default: 10.0 Hz
    # Example: scan at 10 Hz × 10 KB = 100 KB/s
    medium_bandwidth_target_hz: 10.0

  # Per-topic overrides using regex patterns
  # Evaluated in order - first match wins
  # Overrides take precedence over adaptive strategy
  # topic_overrides:
  #   # Example: Camera topics - aggressive rate limiting
  #   - pattern: "^/camera/.*/image_raw$"
  #     strategy: "rate_limit"
  #     target_hz: 2.0
  #
  #   # Example: Control commands - capture everything
  #   - pattern: "^/cmd_vel$"
  #     strategy: "all"
  #
  #   # Example: Diagnostics - only when values change
  #   - pattern: "^/diagnostics"
  #     strategy: "on_change"
  #
  #   # Example: Debug topics - random 10% sampling
  #   - pattern: "^/debug/.*"
  #     strategy: "sample"
  #     sample_percentage: 10.0

tf:
  # ===== TF (TRANSFORM TREE) SNAPSHOTS =====
  #
  # The agent subscribes to /tf and /tf_static to maintain an internal view of the
  # transform tree, and periodically emits a full-tree snapshot for time-series replay.

  # Enable/disable TF monitoring (default: true)
  enabled: true

  # Snapshot frequency (Hz). Default: 10.0
  snapshot_hz: 10.0

  # Maximum number of transforms kept in memory to bound RAM usage. Default: 10000
  max_transforms: 10000

  # Enable verbose TF debugging logs (default: false)
  verbose: false

monitoring:
  # ===== ACTION / SERVICE MONITORING =====
  #
  # These knobs control passive monitoring for actions and services. In production,
  # we expect to use ROS2 tracing for high-fidelity service monitoring; these settings
  # still apply to any subscription/event-based capture to keep memory bounded.

  actions:
    # Enable/disable action monitoring
    enabled: true
    # Capture strategy: all | on_change | rate_limit | sample
    strategy: "rate_limit"
    # Rate limit target when strategy is "rate_limit"
    target_hz: 50.0
    # Sampling percentage when strategy is "sample"
    sample_percentage: 10.0
    # Max bytes to keep per action event payload (excess is truncated)
    max_payload_bytes: 16384

  services:
    # Enable/disable service monitoring
    enabled: true
    # Capture strategy: all | on_change | rate_limit | sample
    strategy: "rate_limit"
    # Rate limit target when strategy is "rate_limit"
    target_hz: 20.0
    # Sampling percentage when strategy is "sample"
    sample_percentage: 10.0
    # Max bytes to keep per service event payload (excess is truncated)
    max_payload_bytes: 65536

logging:
  # Capture ROS2 logs from /rosout topic
  capture_rosout: true

  # Agent log level (trace, debug, info, warn, error)
  level: "info"

  # Default output format for logs: json | human
  format: "json"

  # Output destinations (can enable multiple)
  outputs:
    stdout:
      enabled: true
      # format: "human" # Optional per-sink override
    file:
      enabled: false
      path: "/var/log/robot_ops_agent"
      max_size_mb: 100
      max_files: 5
      # format: "json" # Optional per-sink override
    journald:
      # auto = enable if journald is available
      enabled: "auto" # auto | true | false

  # Per-module filters (override global level for specific targets)
  # module_filters:
  #   "robot_agent::telemetry": "debug"
  #   "robot_agent::subscription": "warn"

  # Additional options
  include_line_numbers: true
  include_thread_ids: false

log_retention:
  # Keep logs until uploaded OR storage limit reached
  delete_after_upload: true
  max_local_storage_mb: 500
  max_age_days: 30

# ===== SYSTEM LOGS VIA JOURNALD =====
#
# Best-effort host system log collection. If journald is not accessible (non-systemd systems,
# container without journal access, insufficient permissions), the agent continues to run
# normally and will only emit a warning by default.
system_logs:
  enabled: true
  on_unavailable: "warn" # warn | ignore

  # Journald configuration
  journald:
    # Minimum priority to capture (emerg, alert, crit, err, warning, notice, info, debug)
    min_priority: "warning"

    # Systemd unit filtering (glob-style)
    units:
      include: ["*.service"]
      exclude:
        - "user@*"
        - "session-*"
        - "systemd-*"

    # Source category filtering
    sources:
      kernel:
        enabled: true
        min_priority: "warning"
      systemd:
        enabled: true
        min_priority: "warning"
      network:
        enabled: true
        min_priority: "warning"
      auth:
        enabled: false # Privacy-sensitive; opt-in only
        min_priority: "warning"

    # Transport filtering (how logs entered journald)
    transports:
      kernel: true
      syslog: true
      stdout: true
      journal: true
      audit: false

    # Rate limiting (prevent log storms)
    rate_limiting:
      max_logs_per_minute: 100
      burst_allowance: 200

    # Deduplication (collapse repeated messages)
    deduplication:
      enabled: true
      window_seconds: 60
